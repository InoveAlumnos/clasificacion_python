{"cells":[{"cell_type":"markdown","metadata":{"id":"67RBrvkUviuj"},"source":["<a href=\"https://www.inove.com.ar\"><img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/PA%20Banner.png\" width=\"1000\" align=\"center\"></a>\n","\n","\n","# Ejercicio de clasificación con Support Vector Machine\n","\n","Ejemplo de clasificación utilizando support vector machine para la clasificación de un dataset sintético y ver como este clasificador divide utilizando su hiper-plano<br>\n","\n","v1.1"]},{"cell_type":"markdown","source":["### Objetivos: \n","*   Preprocesar los datos (descarga, lectura, limplieza y filtrado).\n","*   Conocer como funciona el algoritmo clasificación con  Support Vector Machine.\n","*   Evaluar el resultado el algoritmo clasificación con  Support Vector Machine."],"metadata":{"id":"y4499qxIX6Yy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y2sSeyEovSw-"},"outputs":[],"source":["#Librerias a implementar\n","import os\n","import platform\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"7Szo7P_3v00C"},"source":["# Recolectar datos\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline1.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"markdown","source":["### Código de descarga del dataset"],"metadata":{"id":"lZvT5IMKYDN6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnVpNGuAvyFi"},"outputs":[],"source":["if os.access('dataset_test_clf.csv', os.F_OK) is False:\n","    if platform.system() == 'Windows':\n","        !curl https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/dataset_test_clf.csv > dataset_test_clf.csv\n","    else:\n","        !wget dataset_test_clf.csv https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/dataset_test_clf.csv"]},{"cell_type":"markdown","metadata":{"id":"BbNSgxdfw0ix"},"source":["### `dataset_test_clf.csv`:\n","El dataset **`dataset_test_clf.csv`** es un dataset sintético generado para probar y comprar clasificadores que utilizan un hiper-plano para separar los datos<br>"]},{"cell_type":"markdown","metadata":{"id":"NHHsGe1Qypde"},"source":["# Procesar datos\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline2.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvzaKBMbyoiy"},"outputs":[],"source":["# Una vez descargado el archivo en Colab.\n","# Leerlo con Pandas y el método read_csv\n","# Una vez extraida toda la información se almacena en df\n","# A partir de df y el método describe(), mostrará la descripción estadistica básica del archivo que se guardará en des\n","# Crear una fila nueva llamada Nan en el DataFrame  des,\n","# que indica la cantidad de datos tipo Nan que tiene cada columna.\n","# Para crear una nueva fila, se utilizará el operador loc, donde se indica el nombre\n","# de la nueva fila y con que valores se completará.\n","# La información será de los datos faltantes df.isna().sum()\n","# Crear una fila nueva llamada %Nan en el DataFrame des,\n","# Esta fila se completará con los porcentajes de Nan encontrados en cada columna.\n","df = pd.read_csv(\"dataset_test_clf.csv\")\n","des = df.describe()\n","des.loc['Nan'] = df.isna().sum()\n","des.loc['%Nan'] = (df.isna().mean())*100\n","des"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cw9HbE88y3wu"},"outputs":[],"source":["# Muestra las 5 primeras filas del DataFrame df\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LirgXKpiy8dr"},"outputs":[],"source":["# Cantidad de filas y columnas con shape\n","# En la ubicación 0 corresponde a las filas\n","print('Cantidad de datos en observacion:', df.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"0BnzYdlRzBxz"},"source":["# Explorar datos\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline3.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yH6oDykAzBMG"},"outputs":[],"source":["# Exploramos la distribución del dataset\n","# sns, alias de Seaborn\n","# método scatterplot, gráfico de dispersión\n","# Necesita la data, indicar el nombre de la columna del DataFrame para el eje x y el eje y\n","# También se indica el parámetro hue, que significa que va a discriminar por los valores de y\n","sns.scatterplot(data=df, x=\"x1\", y=\"x2\", hue=\"y\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7z_SuZlj3gbQ"},"source":["# Entrenar modelo\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline4.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"markdown","metadata":{"id":"ntY84fHj3q5q"},"source":["El primer paso es obtener los datos que serán la entrada del sistema (X) y los datos que serán la salida del modelo estimador (y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIg2_OQ43fqZ"},"outputs":[],"source":["# Obtener los valores de X e y\n","# En X se almacenarán todos los valores de las columnas excepto los valores de la columna \"y\"\n","# En y sólo se almacena los valores de la columna \"y\", que será la columna objetivo.\n","# Para ello, se accede a la columna \"y\" del DataFrame df usando corchetes.\n","# En ambos caso, se implementa el método values para obtener solo los valores y que no vengan incluidos los nombres de las columnas.\n","X = df.drop('y', axis=1).values\n","y = df['y'].values"]},{"cell_type":"markdown","metadata":{"id":"sbr-SnON4LuM"},"source":["Siguiente paso es dividir el dataset en entrenamiento (train) y evaluación (test). Utilizaremos el criterio 70%30%"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVD4YkjS4MW2"},"outputs":[],"source":["# Se importa la herramienta de sklearn.model_selectionl como train_test_split\n","from sklearn.model_selection import train_test_split\n","\n","# Fijamos un \"random_state\" constante para que siempre el dataset se parta de la misma forma\n","# para poder repetir los ensayos\n","# Ojo! Los dataset de train y test son array numpy\n","# Se importa la herramienta de la libreria  train_test_split()\n","# Necesita los valores de X e y\n","# test_size=0.3, permite indicar el porcentaje de valores para evaluar, equivalente a un 30%\n","# random_state=42,  es un número fijo que utilizan comunmente en documentación, significa que para cada ejecución del algoritmo \n","#se genere nuevos valores aleatorios\n","# y los conjuntos de datos de entrenamiento y pruebas serán diferentes.\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"wBh2fSnT4SED"},"source":["#### Crear un modelo de clasificación con SVM\n","Kernels que se pueden utilizar (defecto rbf):\n","- linear --> Kernel lineal (igual a la regresion logística)\n","- poly --> Igual que el polinomial de la regresión\n","- Radial basis function (rbf) --> Es el que viene por defecto\n","- sigmoid"]},{"cell_type":"markdown","source":["### Se construyen dos modelos de clasificación para comparar:\n","\n","\n","*   Uno lineal\n","*   Otro polinomial\n","\n"],"metadata":{"id":"-vqF9OjNZ8Bx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRw2jgPl4Xuc"},"outputs":[],"source":["# Se importa la herramienta de sklearn como svm\n","from sklearn import svm\n","\n","# Se crea el objeto clf_linear a partir de la clase svm.SVC \n","# Indicando como parámetro kernel='linear' se utiliza para calcular previamente la matriz del kernel a partir de matrices de datos\n","clf_linear = svm.SVC(kernel='linear')\n","\n","# De clf_linear se accede al método .fit() para entrenar\n","# Luego al método .predict() para predecir\n","clf_linear.fit(X_train, y_train)\n","y_hat_linear = clf_linear.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CLCjMjRDQWLQ"},"outputs":[],"source":["# Se importa la herramienta de sklearn como svm\n","from sklearn import svm\n","\n","# Se crea el objeto clf_linear a partir de la clase svm.SVC \n","# Indicando como parámetro kernel='poly' se utiliza para calcular previamente la matriz del kernel a partir de matrices de datos\n","# y el grado en degree\n","clf_poly = svm.SVC(kernel='poly', degree=3)\n","\n","\n","# De clf_linear se accede al método .fit() para entrenar\n","# Luego al método .predict() para predecir\n","clf_poly.fit(X_train, y_train)\n","y_hat_poly = clf_poly.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"w3IfjUuI4XnD"},"source":["# Validar modelo\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline5.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"markdown","source":["### Se calcula el accuracy para ambos modelos"],"metadata":{"id":"jGqm-jO3ahmY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CMKONtv55zL8"},"outputs":[],"source":["# Calcular la exactitud (accuracy)\n","from sklearn.metrics import accuracy_score\n","print('Linear accuracy:', accuracy_score(y_test, y_hat_linear, normalize=True))\n","print('Poly accuracy:', accuracy_score(y_test, y_hat_poly, normalize=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TeLeYLYz6ZhO"},"outputs":[],"source":["# Se utliza la matriz de confusión para evaluar la precisión de una clasificación.\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","# Necesita dos variables que contengan los valores a comparar\n","cm = confusion_matrix(y_test, y_hat_poly)\n","\n","# Código para realizar la representación gráfica con los resultados\n","# Se crea la varible cmd, que almacena visualization de la Confusion Matrix \n","# Necesita la variable cm que contiene los resultados de la comparación entre los valores reales y predicción\n","# display_labels, se especifica las etiquetas de las categorias que se evalúan.\n","cmd = ConfusionMatrixDisplay(cm, display_labels=clf_poly.classes_)\n","\n","# Con cmd.plot se especifica el mapa de colores reconocido por matplotlib.\n","cmd.plot(cmap=plt.cm.BuPu)\n","\n","# Para mostrar la figura\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9dZxGbjG96jR"},"source":["# Utilizar modelo\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline6.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"code","source":["prueba = pd.DataFrame({'x1':[0.523624], 'x2':[0.145478]})\n","\n","X_prueba = prueba.values\n","\n","y_hat_poly = clf_poly.predict(X_prueba)\n","y_hat_poly"],"metadata":{"id":"nxdZnlTebUX5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h7yzVZcZ9-4m"},"source":["# Conclusión\n","<img src=\"https://raw.githubusercontent.com/InoveAlumnos/dataset_analytics_python/master/images/Pipeline7.png\" width=\"1000\" align=\"middle\">"]},{"cell_type":"markdown","metadata":{"id":"sWAReOgo-B7b"},"source":["Se puede observar SVM nos permite crear un separador no lineal el cual puede ser más flexible y adaptarse mejor a los datos. No hay que abusar de la complejidad del separador para no tener overfitting"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}